# Emotional AI Chatbot Project - Cline Development Rules

## ðŸŽ¯ Project Overview
This project builds an emotional AI chatbot with 3D avatar, voice interaction, and real-time emotion detection using Python backend, Unity frontend, OpenAI GPT-4, and various AI services.

## ðŸ“ File Organization

### Directory Structure
```
Emot/
â”œâ”€â”€ backend/                 # Python Flask/FastAPI server
â”œâ”€â”€ unity_client/           # Unity 3D avatar application
â”œâ”€â”€ models/                 # Local AI models and weights
â”œâ”€â”€ data/                   # Session data and audio cache
â”œâ”€â”€ config/                 # Configuration files
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ tests/                  # Unit and integration tests
â””â”€â”€ scripts/                # Utility and setup scripts
```

### File Naming Conventions
- Python files: `snake_case.py`
- Unity C# scripts: `PascalCase.cs`
- Configuration files: `lowercase.json`, `lowercase.yaml`
- Documentation: `UPPERCASE.md` for main docs, `lowercase.md` for specific docs

## ðŸ Python Code Standards

### Style Guidelines
- Follow PEP 8 strictly
- Use type hints for all function parameters and return values
- Maximum line length: 88 characters (Black formatter standard)
- Use docstrings for all classes and functions (Google style)
- Import order: standard library, third-party, local imports

### Code Structure
```python
"""Module docstring describing purpose."""

import os
import sys
from typing import Dict, List, Optional

import numpy as np
import requests
from flask import Flask

from .local_module import LocalClass


class ExampleClass:
    """Class docstring with purpose and usage."""
    
    def __init__(self, param: str) -> None:
        """Initialize with parameter."""
        self.param = param
    
    def process_data(self, data: Dict[str, Any]) -> Optional[str]:
        """Process data and return result.
        
        Args:
            data: Input data dictionary
            
        Returns:
            Processed result or None if failed
            
        Raises:
            ValueError: If data is invalid
        """
        pass
```

### Error Handling
- Use specific exception types, not bare `except:`
- Log errors with appropriate levels (DEBUG, INFO, WARNING, ERROR, CRITICAL)
- Return meaningful error messages in API responses
- Use try-except blocks for external API calls and file operations

### Configuration Management
- Store API keys in environment variables, never in code
- Use `config/settings.json` for application settings
- Validate configuration on startup
- Provide default values for optional settings

## ðŸŽ® Unity C# Standards

### Naming Conventions
- Classes: `PascalCase`
- Methods: `PascalCase`
- Variables: `camelCase`
- Constants: `UPPER_SNAKE_CASE`
- Private fields: `_camelCase`

### Code Structure
```csharp
using System;
using System.Collections.Generic;
using UnityEngine;

namespace EmotionalAI
{
    /// <summary>
    /// Handles avatar animation and emotion synchronization.
    /// </summary>
    public class AvatarController : MonoBehaviour
    {
        [SerializeField] private Animator _animator;
        [SerializeField] private AudioSource _audioSource;
        
        /// <summary>
        /// Updates avatar emotion based on detected user emotion.
        /// </summary>
        /// <param name="emotion">Detected emotion type</param>
        public void UpdateEmotion(EmotionType emotion)
        {
            // Implementation
        }
    }
}
```

## ðŸŒ API Design Standards

### REST Endpoint Conventions
- Use HTTP verbs correctly (GET, POST, PUT, DELETE)
- Endpoint naming: `/api/v1/resource` or `/api/v1/resource/{id}`
- Use plural nouns for collections: `/api/v1/sessions`
- Use consistent response formats

### Response Format
```json
{
    "success": true,
    "data": {
        "message": "Hello, I understand you're feeling sad today.",
        "emotion": "empathetic",
        "session_id": "uuid-here"
    },
    "error": null,
    "timestamp": "2024-01-01T12:00:00Z"
}
```

### Error Response Format
```json
{
    "success": false,
    "data": null,
    "error": {
        "code": "INVALID_INPUT",
        "message": "Voice input could not be processed",
        "details": "Audio file format not supported"
    },
    "timestamp": "2024-01-01T12:00:00Z"
}
```

## ðŸ”’ Security Guidelines

### API Key Management
- Store in environment variables: `OPENAI_API_KEY`, `ELEVENLABS_API_KEY`
- Never commit API keys to version control
- Use `.env` files for local development
- Validate API keys on application startup

### Data Privacy
- Don't log sensitive user data (voice recordings, personal information)
- Implement session cleanup after inactivity
- Use secure file permissions for session data
- Encrypt stored conversation history

## ðŸ“ Documentation Standards

### Code Comments
- Explain WHY, not WHAT
- Use TODO comments for future improvements: `# TODO: Add emotion intensity scaling`
- Use FIXME for known issues: `# FIXME: Handle edge case when no face detected`

### Function Documentation
```python
def analyze_emotion(frame: np.ndarray, model_path: str) -> Dict[str, float]:
    """Analyze facial emotion from video frame.
    
    Uses MediaPipe Face Mesh to detect facial landmarks and classify
    emotions using a trained model. Returns confidence scores for
    each emotion category.
    
    Args:
        frame: RGB image array from webcam
        model_path: Path to trained emotion classification model
        
    Returns:
        Dictionary mapping emotion names to confidence scores
        Example: {"happy": 0.8, "sad": 0.1, "neutral": 0.1}
        
    Raises:
        FileNotFoundError: If model file doesn't exist
        ValueError: If frame is invalid format
    """
```

## ðŸ§ª Testing Standards

### Test File Organization
- Unit tests: `tests/unit/test_module_name.py`
- Integration tests: `tests/integration/test_feature_name.py`
- Use pytest for Python testing
- Use Unity Test Framework for Unity tests

### Test Naming
```python
def test_emotion_detection_with_happy_face():
    """Test emotion detection correctly identifies happy expression."""
    pass

def test_api_chat_endpoint_returns_valid_response():
    """Test chat API returns properly formatted response."""
    pass
```

## ðŸ Environment Management

### Conda Environment Setup
- Use conda environment named `emotional_ai` with Python 3.11
- Activate environment: `conda activate emotional_ai`
- Install dependencies via pip within the conda environment
- This ensures better compatibility with AI/ML packages

### Package Installation Order
1. Core web framework packages first (Flask, requests, etc.)
2. AI/ML packages (OpenAI, numpy, opencv-python)
3. Specialized packages (whisper, elevenlabs, mediapipe) as needed
4. Development tools last (pytest, black, flake8)

## ðŸ”„ Git Workflow & Best Practices

### Repository Setup
- Use comprehensive `.gitignore` to exclude sensitive files and build artifacts
- Never commit API keys, passwords, or sensitive data
- Keep `.env.template` updated but never commit actual `.env` files
- Use Git LFS for large model files (>100MB)

### Commit Messages
- Use conventional commits format: `type(scope): description`
- Types: `feat`, `fix`, `docs`, `style`, `refactor`, `test`, `chore`
- Examples:
  - `feat(llm): add emotion detection pipeline`
  - `fix(api): resolve session timeout issue`
  - `docs(readme): update setup instructions`
  - `refactor(session): improve database connection handling`

### Branch Strategy
- `main`: Production-ready code, protected branch
- `develop`: Integration branch for features
- `feature/*`: Individual feature development (`feature/voice-processing`)
- `bugfix/*`: Bug fixes (`bugfix/session-cleanup`)
- `hotfix/*`: Critical production fixes
- `release/*`: Release preparation branches

### Branch Naming Conventions
- Use lowercase with hyphens: `feature/emotion-detection`
- Include issue numbers when applicable: `feature/123-voice-integration`
- Keep names descriptive but concise

### Pre-commit Checklist
- [ ] Run tests: `python test_backend.py`
- [ ] Format code: `black .` (if using Black formatter)
- [ ] Lint code: `flake8` (if using Flake8)
- [ ] Update documentation if needed
- [ ] Check `.gitignore` covers new file types
- [ ] Verify no sensitive data in commit
- [ ] Test locally before pushing

### Pull Request Guidelines
- Create descriptive PR titles and descriptions
- Reference related issues: "Closes #123"
- Include testing instructions
- Request appropriate reviewers
- Ensure CI/CD checks pass
- Keep PRs focused and reasonably sized

### File Management
- Keep repository clean with proper `.gitignore`
- Remove unused files and dependencies
- Organize files according to project structure
- Use meaningful file and directory names

### Security Practices
- Never commit secrets, API keys, or credentials
- Use environment variables for configuration
- Regularly rotate API keys and update `.env.template`
- Review commits for accidental sensitive data exposure
- Use `.env.example` or `.env.template` for documentation

### Large Files and Models
- Use Git LFS for files >100MB
- Store large models in `models/` directory
- Document model sources and versions
- Consider external storage for very large datasets

### Collaboration Guidelines
- Sync with `main` branch regularly
- Communicate breaking changes clearly
- Use meaningful commit messages for team understanding
- Tag releases with semantic versioning (v1.0.0)
- Maintain clean commit history (squash when appropriate)

## ðŸ“ˆ Project Roadmap & Development Phases

### Phase 1: Core Backend Infrastructure âœ… COMPLETED
- [x] Project structure and directory organization
- [x] Conda environment setup with Python 3.11
- [x] Flask API server with RESTful endpoints
- [x] GPT-4o-mini integration with emotional context awareness
- [x] Session management with SQLite database and file fallback
- [x] Comprehensive error handling and logging
- [x] Testing framework and automated tests
- [x] Documentation (README, .clinerules, .gitignore)
- [x] Environment configuration and security practices

### Phase 2: Voice Processing Pipeline âœ… COMPLETED
- [x] OpenAI Whisper integration for speech-to-text
- [x] Voice processing API endpoints and real-time processing
- [x] Audio file format handling (WAV, MP3, OGG, WebM)
- [x] Voice emotion modulation based on AI response tone
- [x] Audio caching and cleanup mechanisms
- [x] Coqui TTS integration with Azure fallback
- [x] Streaming audio pipeline foundation

### Phase 2.5: Call Interface & 3D Avatar System ðŸš§ CURRENT PRIORITY
- [ ] Transform chat interface to call interface layout
- [ ] Add "Click to Enter Call" button after AI greeting
- [ ] Implement WebAudio context unlock on user gesture
- [ ] Create large 3D avatar container (16:9 responsive)
- [ ] Set up Three.js scene for VRM 1.0 avatar rendering
- [ ] Integrate Ready Player Me for avatar creation
- [ ] Implement call state machine (IDLE â†’ GREETING â†’ CALL_ACTIVE)
- [ ] Add streaming TTS with real-time lip-sync
- [ ] Implement phoneme-to-viseme mapping (ARKit blendshapes)
- [ ] Add call controls (mute, volume, end call)
- [ ] Optimize for <800ms LLM, <300ms TTS, Â±60ms lip-sync
- [ ] Test complete call experience end-to-end

### Phase 3: Real-time Emotion Detection ðŸ“‹ PLANNED
- [ ] MediaPipe Face Mesh integration
- [ ] Webcam access and video stream processing
- [ ] Facial landmark detection and analysis
- [ ] Emotion classification model (7 basic emotions)
- [ ] Real-time emotion detection pipeline
- [ ] Emotion confidence scoring and filtering
- [ ] Integration with chat API for emotion-aware responses
- [ ] Emotion detection API endpoints

### Phase 4: Unity 3D Avatar System ðŸ“‹ PLANNED
- [ ] Unity 2022.3 LTS project setup
- [ ] Ready Player Me SDK integration
- [ ] 3D avatar creation and customization
- [ ] Avatar animation system and state machine
- [ ] Lip-sync integration with TTS audio
- [ ] Emotion-based facial expressions and gestures
- [ ] Unity-Python REST API client
- [ ] Real-time avatar response synchronization

### Phase 5: Voice Cloning & Advanced Features ðŸ“‹ PLANNED
- [ ] Voice sample collection interface (5-10 minute recordings)
- [ ] Coqui TTS voice cloning model training
- [ ] Consent and privacy management system
- [ ] Voice model storage with encryption
- [ ] Multi-voice selection and management
- [ ] Voice cloning API endpoints
- [ ] Loved ones voice integration
- [ ] Advanced emotion analysis (intensity, mixed emotions)
- [ ] Conversation context and memory improvements
- [ ] User preference learning and adaptation
- [ ] Analytics and usage tracking
- [ ] Deployment configuration and containerization

### Phase 6: Polish & Production Readiness ðŸ“‹ FUTURE
- [ ] Comprehensive testing suite (unit, integration, e2e)
- [ ] Performance benchmarking and optimization
- [ ] Security audit and penetration testing
- [ ] User interface improvements and accessibility
- [ ] Documentation for end users
- [ ] CI/CD pipeline setup
- [ ] Production deployment guides
- [ ] Monitoring and logging infrastructure

## ðŸŽ¯ Current Development Focus

### Immediate Next Steps (Phase 2)
1. **Voice Input Processing**
   - Install and configure OpenAI Whisper
   - Create voice recording and transcription endpoints
   - Implement real-time audio streaming

2. **Voice Output Generation**
   - Set up ElevenLabs API integration
   - Create text-to-speech endpoints with emotion modulation
   - Implement audio playback and streaming

3. **Voice Pipeline Integration**
   - Connect voice input/output with existing chat system
   - Add voice-specific error handling and fallbacks
   - Create comprehensive voice processing tests

### Technical Priorities
- Maintain backward compatibility with existing text-based API
- Ensure real-time performance for voice interactions
- Implement proper audio format handling and conversion
- Add comprehensive logging for voice processing pipeline
- Create fallback mechanisms for voice service failures

### Architecture Considerations
- Keep voice processing modular and loosely coupled
- Design for scalability and concurrent voice sessions
- Implement proper resource management for audio processing
- Consider WebSocket connections for real-time voice streaming
- Plan for future multi-language voice support

## ðŸŽ­ 3D Avatar & Call Interface Standards

### Avatar Specifications
- **Style**: Stylized-realistic (not cartoon, avoids uncanny valley)
- **Format**: VRM 1.0 with humanoid rig and ARKit blendshapes
- **Materials**: PBR (physically-based) skin, hair, eyes for realism
- **Appearance**: Neutral, customizable (face, skin tone, hair)
- **Clothing**: Casual, neutral outfit with studio lighting
- **Animations**: Breathing, blinking, micro-movements for natural presence

### Call Interface Requirements
- **Layout**: 16:9 responsive desktop, 4:3 mobile with letterboxing
- **Avatar Container**: Main screen focus, full-width with proper aspect ratio
- **Call Controls**: Bottom overlay (mute, volume, end call)
- **State Machine**: IDLE â†’ GREETING â†’ CTA_VISIBLE â†’ CALL_ACTIVE
- **Audio Unlock**: "Click to Enter Call" button unlocks WebAudio context
- **Transcript**: Toggle display, default OFF during calls

### Performance Targets
- **LLM First Token**: <800ms response time
- **TTS First Audio**: <300ms after receiving text
- **Avatar Lip-Sync**: Frame-accurate within Â±60ms of audio
- **GPU Usage**: <30% on typical laptop
- **Frame Rate**: Stable 60fps avatar rendering
- **Audio Latency**: <1.2s end-to-end conversation response

### Technical Stack Decisions
- **3D Renderer**: Three.js WebGL for browser compatibility
- **Avatar Format**: VRM 1.0 (Ready Player Me compatible)
- **TTS Primary**: Coqui TTS (self-hosted, cost-effective)
- **TTS Fallback**: Azure/Edge TTS (reliability backup)
- **Lip-Sync**: Phoneme-to-viseme mapping with ARKit blendshapes
- **Audio**: Opus streaming, WAV archival, WebAudio context

## ðŸ”Š Voice Cloning Standards

### Sample Requirements
- **Demo Quality**: 30-60 seconds (basic fidelity, unstable prosody)
- **Good Quality**: 5-10 minutes (stable prosody, recommended v1)
- **Premium Quality**: 30-60 minutes (high fidelity, varied content)
- **Audio Specs**: WAV, mono, 16-48kHz, 24-bit, quiet environment
- **Content**: Phonetically diverse script for better voice quality

### Privacy & Consent Framework
- **Explicit Consent**: Voice owner must provide clear permission
- **Consent Ledger**: Track permissions per voice with timestamps
- **Local-First Storage**: Voice models stored locally when possible
- **Encryption**: All voice data encrypted at rest
- **Easy Deletion**: One-click removal of voice models and data
- **Scoped Usage**: Cloned voices only for intended emotional support
- **No Impersonation**: Clear disclosure when using cloned voices

### Voice Cloning Architecture
- **Training Pipeline**: Coqui TTS voice cloning with fine-tuning
- **Model Storage**: Encrypted, versioned, with metadata
- **TTS Adapter Pattern**: Swappable engines (Coqui, Azure, cloned voices)
- **Quality Tiers**: Multiple fidelity levels based on sample duration
- **Usage Tracking**: Log usage for consent compliance and analytics

## ðŸŽ® Call Experience Standards

### State Management
```
IDLE (avatar visible, chat disabled)
  â†“
GREETING (AI introduces itself)
  â†“
CTA_VISIBLE ("Click to Enter Call" button)
  â†“ (onClick)
CALL_STARTING (WebAudio unlock, TTS warmup)
  â†“
CALL_ACTIVE (conversation mode)
  â†“ (during AI response)
SPEAKING (avatar lip-sync, "Speaking..." indicator)
  â†“ (response complete)
LISTENING (avatar idle, waiting for user)
```

### Audio Pipeline Events
- `assistant_message(text, metadata)` - New AI response
- `tts_stream_start(voice, ssml)` - TTS begins processing
- `tts_audio_chunk(buffer, timestamp)` - Audio data chunk
- `viseme_chunk([{time, visemeId, weight}])` - Lip-sync data
- `tts_end()` - Audio generation complete
- `avatar_state(update)` - Avatar state change

### Browser Compatibility
- **Audio Unlock**: Single user gesture unlocks WebAudio across browsers
- **Safari iOS**: Test audio resume on visibility change
- **Chrome**: Verify autoplay policies compliance
- **Firefox**: Test WebGL performance and audio streaming
- **Mobile**: Optimize for touch interfaces and limited GPU

### Testing Checklist
- [ ] Audio unlock works across Chrome/Safari/Firefox
- [ ] First-token to first-audio <1.2s end-to-end
- [ ] Lip-sync drift under Â±60ms during 30s speech
- [ ] Graceful degradation under 3-5% packet loss
- [ ] GPU usage <30% with dynamic LOD
- [ ] Mobile responsiveness and touch controls
- [ ] Voice cloning consent flow and data protection
